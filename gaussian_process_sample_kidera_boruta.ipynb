{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import sys\n",
    "import re\n",
    "import statistics\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simple_gp\n",
    "from Kidera import kidera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Rowid', 'ID', 'Protein Name', 'Mutation', 'Chain', 'PDB',\n",
      "       'Temperature', 'pH', 'ΔΔG', 'Reference', 'A', 'R', 'N', 'D', 'C', 'Q',\n",
      "       'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V',\n",
      "       'ASP', 'PHE', 'GLN', 'LYS', 'ILE', 'TYR', 'GLY', 'ASN', 'ARG', 'LEU',\n",
      "       'TRP', 'ALA', 'THR', 'VAL', 'HIS', 'CYS', 'GLU', 'MET', 'PRO', 'SER'],\n",
      "      dtype='object')\n",
      "8535\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rowid</th>\n",
       "      <th>ID</th>\n",
       "      <th>Protein Name</th>\n",
       "      <th>Mutation</th>\n",
       "      <th>Chain</th>\n",
       "      <th>PDB</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>pH</th>\n",
       "      <th>ΔΔG</th>\n",
       "      <th>Reference</th>\n",
       "      <th>...</th>\n",
       "      <th>TRP</th>\n",
       "      <th>ALA</th>\n",
       "      <th>THR</th>\n",
       "      <th>VAL</th>\n",
       "      <th>HIS</th>\n",
       "      <th>CYS</th>\n",
       "      <th>GLU</th>\n",
       "      <th>MET</th>\n",
       "      <th>PRO</th>\n",
       "      <th>SER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Tryptophan synthase alpha chain</td>\n",
       "      <td>E49M</td>\n",
       "      <td>A</td>\n",
       "      <td>1WQ5</td>\n",
       "      <td>298.95</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.60</td>\n",
       "      <td>PMID: 378988</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Tryptophan synthase alpha chain</td>\n",
       "      <td>E49Q</td>\n",
       "      <td>A</td>\n",
       "      <td>1WQ5</td>\n",
       "      <td>298.95</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-2.50</td>\n",
       "      <td>PMID: 378988</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Endolysin</td>\n",
       "      <td>W138Y</td>\n",
       "      <td>A</td>\n",
       "      <td>2LZM</td>\n",
       "      <td>298.15</td>\n",
       "      <td>2.2</td>\n",
       "      <td>-1.71</td>\n",
       "      <td>PMID: 911878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>Cellular tumor antigen p53</td>\n",
       "      <td>C242S</td>\n",
       "      <td>A</td>\n",
       "      <td>1TUP</td>\n",
       "      <td>283.15</td>\n",
       "      <td>7.2</td>\n",
       "      <td>-3.07</td>\n",
       "      <td>PMID: 1203434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>Cellular tumor antigen p53</td>\n",
       "      <td>F134L</td>\n",
       "      <td>A</td>\n",
       "      <td>1TUP</td>\n",
       "      <td>283.15</td>\n",
       "      <td>7.2</td>\n",
       "      <td>-4.78</td>\n",
       "      <td>PMID: 1203434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8530</th>\n",
       "      <td>8570</td>\n",
       "      <td>13318</td>\n",
       "      <td>Transcriptional repressor arc</td>\n",
       "      <td>V25A</td>\n",
       "      <td>A</td>\n",
       "      <td>1ARR</td>\n",
       "      <td>298.15</td>\n",
       "      <td>7.5</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>PMID: 7664079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8531</th>\n",
       "      <td>8571</td>\n",
       "      <td>13319</td>\n",
       "      <td>Transcriptional repressor arc</td>\n",
       "      <td>V18A</td>\n",
       "      <td>A</td>\n",
       "      <td>1ARR</td>\n",
       "      <td>298.15</td>\n",
       "      <td>7.5</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>PMID: 7664079</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8532</th>\n",
       "      <td>8572</td>\n",
       "      <td>13320</td>\n",
       "      <td>Transcriptional repressor arc</td>\n",
       "      <td>V33A</td>\n",
       "      <td>A</td>\n",
       "      <td>1ARR</td>\n",
       "      <td>298.15</td>\n",
       "      <td>7.5</td>\n",
       "      <td>-2.10</td>\n",
       "      <td>PMID: 7664079</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8533</th>\n",
       "      <td>8573</td>\n",
       "      <td>13321</td>\n",
       "      <td>Transcriptional repressor arc</td>\n",
       "      <td>S32A</td>\n",
       "      <td>A</td>\n",
       "      <td>1ARR</td>\n",
       "      <td>298.15</td>\n",
       "      <td>7.5</td>\n",
       "      <td>-3.80</td>\n",
       "      <td>PMID: 7664079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8534</th>\n",
       "      <td>8574</td>\n",
       "      <td>13323</td>\n",
       "      <td>Cyclin-dependent kinase inhibitor 2A</td>\n",
       "      <td>W15D</td>\n",
       "      <td>A</td>\n",
       "      <td>1A5E</td>\n",
       "      <td>293.15</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.17</td>\n",
       "      <td>PMID: 12614625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8535 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Rowid     ID                          Protein Name Mutation Chain   PDB  \\\n",
       "0         1      1       Tryptophan synthase alpha chain     E49M     A  1WQ5   \n",
       "1         2      2       Tryptophan synthase alpha chain     E49Q     A  1WQ5   \n",
       "2         3      3                             Endolysin    W138Y     A  2LZM   \n",
       "3         4      6            Cellular tumor antigen p53    C242S     A  1TUP   \n",
       "4         5      7            Cellular tumor antigen p53    F134L     A  1TUP   \n",
       "...     ...    ...                                   ...      ...   ...   ...   \n",
       "8530   8570  13318         Transcriptional repressor arc     V25A     A  1ARR   \n",
       "8531   8571  13319         Transcriptional repressor arc     V18A     A  1ARR   \n",
       "8532   8572  13320         Transcriptional repressor arc     V33A     A  1ARR   \n",
       "8533   8573  13321         Transcriptional repressor arc     S32A     A  1ARR   \n",
       "8534   8574  13323  Cyclin-dependent kinase inhibitor 2A     W15D     A  1A5E   \n",
       "\n",
       "      Temperature   pH   ΔΔG       Reference  ...  TRP  ALA  THR  VAL  HIS  \\\n",
       "0          298.95  7.0  4.60    PMID: 378988  ...  0.0  4.0  2.0  4.0  0.0   \n",
       "1          298.95  7.0 -2.50    PMID: 378988  ...  0.0  4.0  2.0  4.0  0.0   \n",
       "2          298.15  2.2 -1.71    PMID: 911878  ...  0.0  5.0  4.0  4.0  0.0   \n",
       "3          283.15  7.2 -3.07   PMID: 1203434  ...  0.0  0.0  0.0  1.0  2.0   \n",
       "4          283.15  7.2 -4.78   PMID: 1203434  ...  0.0  2.0  5.0  3.0  0.0   \n",
       "...           ...  ...   ...             ...  ...  ...  ...  ...  ...  ...   \n",
       "8530       298.15  7.5 -0.40   PMID: 7664079  ...  0.0  1.0  0.0  3.0  0.0   \n",
       "8531       298.15  7.5 -0.50   PMID: 7664079  ...  1.0  1.0  0.0  3.0  0.0   \n",
       "8532       298.15  7.5 -2.10   PMID: 7664079  ...  1.0  1.0  0.0  4.0  0.0   \n",
       "8533       298.15  7.5 -3.80   PMID: 7664079  ...  0.0  1.0  0.0  3.0  0.0   \n",
       "8534       293.15  8.5  0.17  PMID: 12614625  ...  0.0  9.0  1.0  4.0  0.0   \n",
       "\n",
       "      CYS  GLU  MET  PRO  SER  \n",
       "0     0.0  0.0  1.0  2.0  2.0  \n",
       "1     0.0  0.0  1.0  2.0  2.0  \n",
       "2     0.0  2.0  2.0  1.0  2.0  \n",
       "3     4.0  2.0  3.0  1.0  2.0  \n",
       "4     5.0  4.0  1.0  3.0  3.0  \n",
       "...   ...  ...  ...  ...  ...  \n",
       "8530  0.0  4.0  0.0  0.0  1.0  \n",
       "8531  0.0  2.0  0.0  1.0  0.0  \n",
       "8532  0.0  3.0  0.0  0.0  2.0  \n",
       "8533  0.0  3.0  0.0  0.0  1.0  \n",
       "8534  0.0  3.0  1.0  3.0  2.0  \n",
       "\n",
       "[8535 rows x 50 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/DDG_Dataset.csv')\n",
    "# df = pd.read_csv('data/DDG_Dataset_mini.csv')\n",
    "df = df[df['Temperature'] != \"'-\"].reset_index(drop=True)\n",
    "df['Temperature'] = df['Temperature'].astype('float64')\n",
    "ndata = df.shape[0]\n",
    "print(df.columns)\n",
    "print(ndata)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ALA</th>\n",
       "      <td>-1.56</td>\n",
       "      <td>-1.67</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARG</th>\n",
       "      <td>0.22</td>\n",
       "      <td>1.27</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1.87</td>\n",
       "      <td>-1.70</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.92</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASN</th>\n",
       "      <td>1.14</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.37</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.10</td>\n",
       "      <td>-1.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASP</th>\n",
       "      <td>0.58</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-1.58</td>\n",
       "      <td>0.81</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-1.52</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CYS</th>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-1.05</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>2.41</td>\n",
       "      <td>1.52</td>\n",
       "      <td>-0.69</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GLN</th>\n",
       "      <td>-0.47</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.84</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-2.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GLU</th>\n",
       "      <td>-1.45</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-1.61</td>\n",
       "      <td>1.17</td>\n",
       "      <td>-1.31</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GLY</th>\n",
       "      <td>1.46</td>\n",
       "      <td>-1.96</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>1.32</td>\n",
       "      <td>2.36</td>\n",
       "      <td>-1.66</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HIS</th>\n",
       "      <td>-0.41</td>\n",
       "      <td>0.52</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.61</td>\n",
       "      <td>1.01</td>\n",
       "      <td>-1.85</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ILE</th>\n",
       "      <td>-0.73</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>1.79</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.66</td>\n",
       "      <td>-1.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LEU</th>\n",
       "      <td>-1.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>-2.05</td>\n",
       "      <td>0.96</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LYS</th>\n",
       "      <td>-0.34</td>\n",
       "      <td>0.82</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.54</td>\n",
       "      <td>-1.62</td>\n",
       "      <td>1.15</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MET</th>\n",
       "      <td>-1.40</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-1.27</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PHE</th>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.98</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>-1.43</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.71</td>\n",
       "      <td>-0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRO</th>\n",
       "      <td>2.06</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>0.88</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-2.30</td>\n",
       "      <td>0.74</td>\n",
       "      <td>-0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SER</th>\n",
       "      <td>0.81</td>\n",
       "      <td>-1.08</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.42</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>-1.89</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>-0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THR</th>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.63</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRP</th>\n",
       "      <td>0.30</td>\n",
       "      <td>2.10</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>-1.57</td>\n",
       "      <td>-1.16</td>\n",
       "      <td>0.57</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-2.30</td>\n",
       "      <td>-0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TYR</th>\n",
       "      <td>1.38</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>1.03</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VAL</th>\n",
       "      <td>-0.74</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>2.04</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>-1.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       1     2     3     4     5     6     7     8     9     10\n",
       "0                                                              \n",
       "ALA -1.56 -1.67 -0.97 -0.27 -0.93 -0.78 -0.20 -0.08  0.21 -0.48\n",
       "ARG  0.22  1.27  1.37  1.87 -1.70  0.46  0.92 -0.39  0.23  0.93\n",
       "ASN  1.14 -0.07 -0.12  0.81  0.18  0.37 -0.09  1.23  1.10 -1.73\n",
       "ASP  0.58 -0.22 -1.58  0.81 -0.92  0.15 -1.52  0.47  0.76  0.70\n",
       "CYS  0.12 -0.89  0.45 -1.05 -0.71  2.41  1.52 -0.69  1.13  1.10\n",
       "GLN -0.47  0.24  0.07  1.10  1.10  0.59  0.84 -0.71 -0.03 -2.33\n",
       "GLU -1.45  0.19 -1.61  1.17 -1.31  0.40  0.04  0.38 -0.35 -0.12\n",
       "GLY  1.46 -1.96 -0.23 -0.16  0.10 -0.11  1.32  2.36 -1.66  0.46\n",
       "HIS -0.41  0.52 -0.28  0.28  1.61  1.01 -1.85  0.47  1.13  1.63\n",
       "ILE -0.73 -0.16  1.79 -0.77 -0.54  0.03 -0.83  0.51  0.66 -1.78\n",
       "LEU -1.04  0.00 -0.24 -1.10 -0.55 -2.05  0.96 -0.76  0.45  0.93\n",
       "LYS -0.34  0.82 -0.23  1.70  1.54 -1.62  1.15 -0.08 -0.48  0.60\n",
       "MET -1.40  0.18 -0.42 -0.73  2.00  1.52  0.26  0.11 -1.27  0.27\n",
       "PHE -0.21  0.98 -0.36 -1.43  0.22 -0.81  0.67  1.10  1.71 -0.44\n",
       "PRO  2.06 -0.33 -1.15 -0.75  0.88 -0.45  0.30 -2.30  0.74 -0.28\n",
       "SER  0.81 -1.08  0.16  0.42 -0.21 -0.43 -1.89 -1.15 -0.97 -0.23\n",
       "THR  0.26 -0.70  1.21  0.63 -0.10  0.21  0.24 -1.15 -0.56  0.19\n",
       "TRP  0.30  2.10 -0.72 -1.57 -1.16  0.57 -0.48 -0.40 -2.30 -0.60\n",
       "TYR  1.38  1.48  0.80 -0.56 -0.00 -0.68 -0.31  1.03 -0.05  0.53\n",
       "VAL -0.74 -0.71  2.04 -0.40  0.50 -0.81 -1.07  0.06 -0.46  0.65"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kidera.symbol_lookup\n",
    "kidera_table = pd.read_csv('Kidera/kidera.csv', header=None).set_index(0)\n",
    "kidera_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45.5 s, sys: 28 ms, total: 45.5 s\n",
      "Wall time: 45.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-11.27, -15.66,  15.32, ...,   3.88,   6.42,  -5.23],\n",
       "       [-11.27, -15.66,  15.32, ...,   3.88,   6.42,  -5.23],\n",
       "       [ -6.78,  -6.53,   6.59, ...,   0.97,   2.29,  -6.09],\n",
       "       ...,\n",
       "       [ -5.25,  -0.13,   5.73, ...,   1.19,  -3.48,   1.  ],\n",
       "       [ -2.96,  -2.31,   5.41, ...,   4.96,  -1.61,  -3.84],\n",
       "       [-17.09, -19.41,  -7.  , ..., -11.25,   1.04,   4.88]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "around_acid = np.zeros((ndata, 10))\n",
    "for index, row in df.iterrows():\n",
    "    # print(index)\n",
    "    # print(row)\n",
    "    for acid, r in kidera_table.iterrows():\n",
    "        # print(acid)\n",
    "        # print(r)\n",
    "        # print(row[acid])\n",
    "        # print(row[acid] * r)\n",
    "        around_acid[index, :] += row[acid] * r\n",
    "        # break\n",
    "    # break\n",
    "    \n",
    "around_acid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[298.95,   7.  ,   0.  , ...,   3.88,   6.42,  -5.23],\n",
       "       [298.95,   7.  ,   0.  , ...,   3.88,   6.42,  -5.23],\n",
       "       [298.15,   2.2 ,   0.  , ...,   0.97,   2.29,  -6.09],\n",
       "       ...,\n",
       "       [298.15,   7.5 ,   1.  , ...,   1.19,  -3.48,   1.  ],\n",
       "       [298.15,   7.5 ,   1.  , ...,   4.96,  -1.61,  -3.84],\n",
       "       [293.15,   8.5 ,   0.  , ..., -11.25,   1.04,   4.88]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x = df.iloc[:, :30].drop(columns=['Rowid', 'ID', 'Protein Name', 'Mutation', 'Chain', 'PDB', 'ΔΔG', 'Reference']).to_numpy()\n",
    "data_x = np.concatenate([data_x, around_acid], 1)\n",
    "data_y = df['ΔΔG'].to_numpy()\n",
    "data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.unique(data_x[:, 0]))\n",
    "# print(len(np.unique(data_x[:, 0])))\n",
    "# random.choices(sorted(np.unique(data_x[:, 0])), k=200)\n",
    "# random.choices(range(2), k=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:240: UserWarning: This overload of addcmul is deprecated:\n",
      "\taddcmul(Tensor input, Number value, Tensor tensor1, Tensor tensor2, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul(Tensor input, Tensor tensor1, Tensor tensor2, *, Number value, Tensor out) (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629427478/work/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  residual = torch.addcmul(residual, -1, alpha, mvms, out=residual)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10/100 - Loss: 2.29345\n",
      "Iter 20/100 - Loss: 2.16014\n",
      "Iter 30/100 - Loss: 2.13267\n",
      "Iter 40/100 - Loss: 2.11091\n",
      "Iter 50/100 - Loss: 2.09760\n",
      "Iter 60/100 - Loss: 2.08293\n",
      "Iter 70/100 - Loss: 2.07858\n",
      "Iter 80/100 - Loss: 2.07247\n",
      "Iter 90/100 - Loss: 2.06962\n",
      "Iter 100/100 - Loss: 2.07158\n",
      "Iter 10/100 - Loss: 2.28653\n",
      "Iter 20/100 - Loss: 2.15688\n",
      "Iter 30/100 - Loss: 2.12990\n",
      "Iter 40/100 - Loss: 2.10919\n",
      "Iter 50/100 - Loss: 2.09390\n",
      "Iter 60/100 - Loss: 2.08236\n",
      "Iter 70/100 - Loss: 2.07443\n",
      "Iter 80/100 - Loss: 2.07074\n",
      "Iter 90/100 - Loss: 2.07189\n",
      "Iter 100/100 - Loss: 2.06967\n",
      "Iter 10/100 - Loss: 2.29321\n",
      "Iter 20/100 - Loss: 2.15880\n",
      "Iter 30/100 - Loss: 2.13278\n",
      "Iter 40/100 - Loss: 2.11550\n",
      "Iter 50/100 - Loss: 2.09975\n",
      "Iter 60/100 - Loss: 2.08412\n",
      "Iter 70/100 - Loss: 2.07688\n",
      "Iter 80/100 - Loss: 2.07436\n",
      "Iter 90/100 - Loss: 2.06696\n",
      "Iter 100/100 - Loss: 2.06578\n",
      "Iter 10/100 - Loss: 2.28055\n",
      "Iter 20/100 - Loss: 2.14993\n",
      "Iter 30/100 - Loss: 2.12146\n",
      "Iter 40/100 - Loss: 2.09890\n",
      "Iter 50/100 - Loss: 2.08420\n",
      "Iter 60/100 - Loss: 2.07006\n",
      "Iter 70/100 - Loss: 2.06021\n",
      "Iter 80/100 - Loss: 2.05685\n",
      "Iter 90/100 - Loss: 2.05317\n",
      "Iter 100/100 - Loss: 2.05133\n",
      "Iter 10/100 - Loss: 2.31297\n",
      "Iter 20/100 - Loss: 2.17052\n",
      "Iter 30/100 - Loss: 2.14083\n",
      "Iter 40/100 - Loss: 2.11620\n",
      "Iter 50/100 - Loss: 2.10189\n",
      "Iter 60/100 - Loss: 2.09268\n",
      "Iter 70/100 - Loss: 2.08063\n",
      "Iter 80/100 - Loss: 2.07926\n",
      "Iter 90/100 - Loss: 2.07555\n",
      "Iter 100/100 - Loss: 2.07559\n",
      "Iter 10/100 - Loss: 2.20620\n",
      "Iter 20/100 - Loss: 2.08416\n",
      "Iter 30/100 - Loss: 2.06116\n",
      "Iter 40/100 - Loss: 2.04440\n",
      "Iter 50/100 - Loss: 2.03052\n",
      "Iter 60/100 - Loss: 2.02241\n",
      "Iter 70/100 - Loss: 2.01763\n",
      "Iter 80/100 - Loss: 2.00846\n",
      "Iter 90/100 - Loss: 2.00681\n",
      "Iter 100/100 - Loss: 1.99882\n",
      "Iter 10/100 - Loss: 2.20210\n",
      "Iter 20/100 - Loss: 2.08443\n",
      "Iter 30/100 - Loss: 2.06366\n",
      "Iter 40/100 - Loss: 2.04818\n",
      "Iter 50/100 - Loss: 2.03490\n",
      "Iter 60/100 - Loss: 2.02879\n",
      "Iter 70/100 - Loss: 2.02225\n",
      "Iter 80/100 - Loss: 2.01546\n",
      "Iter 90/100 - Loss: 2.01373\n",
      "Iter 100/100 - Loss: 2.00494\n",
      "Iter 10/100 - Loss: 2.21204\n",
      "Iter 20/100 - Loss: 2.08991\n",
      "Iter 30/100 - Loss: 2.07125\n",
      "Iter 40/100 - Loss: 2.05460\n",
      "Iter 50/100 - Loss: 2.04285\n",
      "Iter 60/100 - Loss: 2.03500\n",
      "Iter 70/100 - Loss: 2.02745\n",
      "Iter 80/100 - Loss: 2.02397\n",
      "Iter 90/100 - Loss: 2.02136\n",
      "Iter 100/100 - Loss: 2.01733\n",
      "Iter 10/100 - Loss: 2.20219\n",
      "Iter 20/100 - Loss: 2.08190\n",
      "Iter 30/100 - Loss: 2.05891\n",
      "Iter 40/100 - Loss: 2.04222\n",
      "Iter 50/100 - Loss: 2.03166\n",
      "Iter 60/100 - Loss: 2.02357\n",
      "Iter 70/100 - Loss: 2.01650\n",
      "Iter 80/100 - Loss: 2.00876\n",
      "Iter 90/100 - Loss: 2.00515\n",
      "Iter 100/100 - Loss: 2.00076\n",
      "Iter 10/100 - Loss: 2.22607\n",
      "Iter 20/100 - Loss: 2.09850\n",
      "Iter 30/100 - Loss: 2.07621\n",
      "Iter 40/100 - Loss: 2.05797\n",
      "Iter 50/100 - Loss: 2.04756\n",
      "Iter 60/100 - Loss: 2.03831\n",
      "Iter 70/100 - Loss: 2.03322\n",
      "Iter 80/100 - Loss: 2.02647\n",
      "Iter 90/100 - Loss: 2.02270\n",
      "Iter 100/100 - Loss: 2.02004\n",
      "Iter 10/100 - Loss: 2.16351\n",
      "Iter 20/100 - Loss: 2.04512\n",
      "Iter 30/100 - Loss: 2.02585\n",
      "Iter 40/100 - Loss: 2.01392\n",
      "Iter 50/100 - Loss: 2.00666\n",
      "Iter 60/100 - Loss: 1.99932\n",
      "Iter 70/100 - Loss: 1.99150\n",
      "Iter 80/100 - Loss: 1.99141\n",
      "Iter 90/100 - Loss: 1.99025\n",
      "Iter 100/100 - Loss: 1.99083\n",
      "Iter 10/100 - Loss: 2.16003\n",
      "Iter 20/100 - Loss: 2.04886\n",
      "Iter 30/100 - Loss: 2.02861\n",
      "Iter 40/100 - Loss: 2.01757\n",
      "Iter 50/100 - Loss: 2.00954\n",
      "Iter 60/100 - Loss: 2.00361\n",
      "Iter 70/100 - Loss: 1.99700\n",
      "Iter 80/100 - Loss: 1.99230\n",
      "Iter 90/100 - Loss: 1.99347\n",
      "Iter 100/100 - Loss: 1.99013\n",
      "Iter 10/100 - Loss: 2.17147\n",
      "Iter 20/100 - Loss: 2.05308\n",
      "Iter 30/100 - Loss: 2.03643\n",
      "Iter 40/100 - Loss: 2.02554\n",
      "Iter 50/100 - Loss: 2.01361\n",
      "Iter 60/100 - Loss: 2.00945\n",
      "Iter 70/100 - Loss: 2.00362\n",
      "Iter 80/100 - Loss: 2.00572\n",
      "Iter 90/100 - Loss: 2.00139\n",
      "Iter 100/100 - Loss: 2.00388\n",
      "Iter 10/100 - Loss: 2.16077\n",
      "Iter 20/100 - Loss: 2.04284\n",
      "Iter 30/100 - Loss: 2.02661\n",
      "Iter 40/100 - Loss: 2.01091\n",
      "Iter 50/100 - Loss: 2.00248\n",
      "Iter 60/100 - Loss: 1.99569\n",
      "Iter 70/100 - Loss: 1.98979\n",
      "Iter 80/100 - Loss: 1.98804\n",
      "Iter 90/100 - Loss: 1.98792\n",
      "Iter 100/100 - Loss: 1.98275\n",
      "Iter 10/100 - Loss: 2.18316\n",
      "Iter 20/100 - Loss: 2.05823\n",
      "Iter 30/100 - Loss: 2.04111\n",
      "Iter 40/100 - Loss: 2.02477\n",
      "Iter 50/100 - Loss: 2.01900\n",
      "Iter 60/100 - Loss: 2.01301\n",
      "Iter 70/100 - Loss: 2.00890\n",
      "Iter 80/100 - Loss: 2.00436\n",
      "Iter 90/100 - Loss: 2.00249\n",
      "Iter 100/100 - Loss: 2.00056\n",
      "Iter 10/100 - Loss: 2.17043\n",
      "Iter 20/100 - Loss: 2.04517\n",
      "Iter 30/100 - Loss: 2.02673\n",
      "Iter 40/100 - Loss: 2.01184\n",
      "Iter 50/100 - Loss: 2.00363\n",
      "Iter 60/100 - Loss: 1.99849\n",
      "Iter 70/100 - Loss: 1.99280\n",
      "Iter 80/100 - Loss: 1.98893\n",
      "Iter 90/100 - Loss: 1.98839\n",
      "Iter 100/100 - Loss: 1.98448\n",
      "Iter 10/100 - Loss: 2.16284\n",
      "Iter 20/100 - Loss: 2.04552\n",
      "Iter 30/100 - Loss: 2.02482\n",
      "Iter 40/100 - Loss: 2.01358\n",
      "Iter 50/100 - Loss: 2.00447\n",
      "Iter 60/100 - Loss: 1.99943\n",
      "Iter 70/100 - Loss: 1.99661\n",
      "Iter 80/100 - Loss: 1.99399\n",
      "Iter 90/100 - Loss: 1.99254\n",
      "Iter 100/100 - Loss: 1.98866\n",
      "Iter 10/100 - Loss: 2.17430\n",
      "Iter 20/100 - Loss: 2.05189\n",
      "Iter 30/100 - Loss: 2.03267\n",
      "Iter 40/100 - Loss: 2.02188\n",
      "Iter 50/100 - Loss: 2.01352\n",
      "Iter 60/100 - Loss: 2.00689\n",
      "Iter 70/100 - Loss: 2.00190\n",
      "Iter 80/100 - Loss: 1.99760\n",
      "Iter 90/100 - Loss: 1.99806\n",
      "Iter 100/100 - Loss: 1.99863\n",
      "Iter 10/100 - Loss: 2.16374\n",
      "Iter 20/100 - Loss: 2.04582\n",
      "Iter 30/100 - Loss: 2.02538\n",
      "Iter 40/100 - Loss: 2.01212\n",
      "Iter 50/100 - Loss: 2.00217\n",
      "Iter 60/100 - Loss: 1.99446\n",
      "Iter 70/100 - Loss: 1.98773\n",
      "Iter 80/100 - Loss: 1.98671\n",
      "Iter 90/100 - Loss: 1.98377\n",
      "Iter 100/100 - Loss: 1.97993\n",
      "Iter 10/100 - Loss: 2.18696\n",
      "Iter 20/100 - Loss: 2.05664\n",
      "Iter 30/100 - Loss: 2.03953\n",
      "Iter 40/100 - Loss: 2.02612\n",
      "Iter 50/100 - Loss: 2.01293\n",
      "Iter 60/100 - Loss: 2.00893\n",
      "Iter 70/100 - Loss: 2.00508\n",
      "Iter 80/100 - Loss: 1.99861\n",
      "Iter 90/100 - Loss: 2.00064\n",
      "Iter 100/100 - Loss: 1.99798\n",
      "Iter 10/100 - Loss: 2.17050\n",
      "Iter 20/100 - Loss: 2.04481\n",
      "Iter 30/100 - Loss: 2.02625\n",
      "Iter 40/100 - Loss: 2.01125\n",
      "Iter 50/100 - Loss: 2.00620\n",
      "Iter 60/100 - Loss: 1.99654\n",
      "Iter 70/100 - Loss: 1.99263\n",
      "Iter 80/100 - Loss: 1.99106\n",
      "Iter 90/100 - Loss: 1.98551\n",
      "Iter 100/100 - Loss: 1.98591\n",
      "Iter 10/100 - Loss: 2.16149\n",
      "Iter 20/100 - Loss: 2.04531\n",
      "Iter 30/100 - Loss: 2.02670\n",
      "Iter 40/100 - Loss: 2.01308\n",
      "Iter 50/100 - Loss: 2.00388\n",
      "Iter 60/100 - Loss: 1.99982\n",
      "Iter 70/100 - Loss: 1.98988\n",
      "Iter 80/100 - Loss: 1.99201\n",
      "Iter 90/100 - Loss: 1.99017\n",
      "Iter 100/100 - Loss: 1.98610\n",
      "Iter 10/100 - Loss: 2.17197\n",
      "Iter 20/100 - Loss: 2.05133\n",
      "Iter 30/100 - Loss: 2.03247\n",
      "Iter 40/100 - Loss: 2.01968\n",
      "Iter 50/100 - Loss: 2.01022\n",
      "Iter 60/100 - Loss: 2.00343\n",
      "Iter 70/100 - Loss: 2.00148\n",
      "Iter 80/100 - Loss: 2.00020\n",
      "Iter 90/100 - Loss: 1.99644\n",
      "Iter 100/100 - Loss: 1.99863\n",
      "Iter 10/100 - Loss: 2.16076\n",
      "Iter 20/100 - Loss: 2.04121\n",
      "Iter 30/100 - Loss: 2.02256\n",
      "Iter 40/100 - Loss: 2.00869\n",
      "Iter 50/100 - Loss: 1.99970\n",
      "Iter 60/100 - Loss: 1.99230\n",
      "Iter 70/100 - Loss: 1.98679\n",
      "Iter 80/100 - Loss: 1.98571\n",
      "Iter 90/100 - Loss: 1.98291\n",
      "Iter 100/100 - Loss: 1.98003\n",
      "Iter 10/100 - Loss: 2.18441\n",
      "Iter 20/100 - Loss: 2.05709\n",
      "Iter 30/100 - Loss: 2.03778\n",
      "Iter 40/100 - Loss: 2.02451\n",
      "Iter 50/100 - Loss: 2.01784\n",
      "Iter 60/100 - Loss: 2.01221\n",
      "Iter 70/100 - Loss: 2.00445\n",
      "Iter 80/100 - Loss: 2.00471\n",
      "Iter 90/100 - Loss: 1.99906\n",
      "Iter 100/100 - Loss: 1.99647\n",
      "Iter 10/100 - Loss: 2.16769\n",
      "Iter 20/100 - Loss: 2.04573\n",
      "Iter 30/100 - Loss: 2.02669\n",
      "Iter 40/100 - Loss: 2.01528\n",
      "Iter 50/100 - Loss: 2.00676\n",
      "Iter 60/100 - Loss: 1.99738\n",
      "Iter 70/100 - Loss: 1.99330\n",
      "Iter 80/100 - Loss: 1.98999\n",
      "Iter 90/100 - Loss: 1.98866\n",
      "Iter 100/100 - Loss: 1.98898\n",
      "Iter 10/100 - Loss: 2.16056\n",
      "Iter 20/100 - Loss: 2.04366\n",
      "Iter 30/100 - Loss: 2.02756\n",
      "Iter 40/100 - Loss: 2.01592\n",
      "Iter 50/100 - Loss: 2.00805\n",
      "Iter 60/100 - Loss: 1.99974\n",
      "Iter 70/100 - Loss: 1.99639\n",
      "Iter 80/100 - Loss: 1.99525\n",
      "Iter 90/100 - Loss: 1.99221\n",
      "Iter 100/100 - Loss: 1.98956\n",
      "Iter 10/100 - Loss: 2.17303\n",
      "Iter 20/100 - Loss: 2.05333\n",
      "Iter 30/100 - Loss: 2.03614\n",
      "Iter 40/100 - Loss: 2.02315\n",
      "Iter 50/100 - Loss: 2.01196\n",
      "Iter 60/100 - Loss: 2.00887\n",
      "Iter 70/100 - Loss: 2.00455\n",
      "Iter 80/100 - Loss: 2.00518\n",
      "Iter 90/100 - Loss: 1.99937\n",
      "Iter 100/100 - Loss: 1.99618\n",
      "Iter 10/100 - Loss: 2.15846\n",
      "Iter 20/100 - Loss: 2.04172\n",
      "Iter 30/100 - Loss: 2.02200\n",
      "Iter 40/100 - Loss: 2.00934\n",
      "Iter 50/100 - Loss: 2.00070\n",
      "Iter 60/100 - Loss: 1.99121\n",
      "Iter 70/100 - Loss: 1.98844\n",
      "Iter 80/100 - Loss: 1.98238\n",
      "Iter 90/100 - Loss: 1.98150\n",
      "Iter 100/100 - Loss: 1.97952\n",
      "Iter 10/100 - Loss: 2.18401\n",
      "Iter 20/100 - Loss: 2.05864\n",
      "Iter 30/100 - Loss: 2.03697\n",
      "Iter 40/100 - Loss: 2.02735\n",
      "Iter 50/100 - Loss: 2.01687\n",
      "Iter 60/100 - Loss: 2.01349\n",
      "Iter 70/100 - Loss: 2.00823\n",
      "Iter 80/100 - Loss: 2.00424\n",
      "Iter 90/100 - Loss: 2.00066\n",
      "Iter 100/100 - Loss: 1.99919\n",
      "Iter 10/100 - Loss: 2.16653\n",
      "Iter 20/100 - Loss: 2.04460\n",
      "Iter 30/100 - Loss: 2.02673\n",
      "Iter 40/100 - Loss: 2.01262\n",
      "Iter 50/100 - Loss: 2.00344\n",
      "Iter 60/100 - Loss: 2.00045\n",
      "Iter 70/100 - Loss: 1.99730\n",
      "Iter 80/100 - Loss: 1.99261\n",
      "Iter 90/100 - Loss: 1.98693\n",
      "Iter 100/100 - Loss: 1.98857\n",
      "Iter 10/100 - Loss: 2.16310\n",
      "Iter 20/100 - Loss: 2.04712\n",
      "Iter 30/100 - Loss: 2.02933\n",
      "Iter 40/100 - Loss: 2.01509\n",
      "Iter 50/100 - Loss: 2.00489\n",
      "Iter 60/100 - Loss: 1.99831\n",
      "Iter 70/100 - Loss: 1.99436\n",
      "Iter 80/100 - Loss: 1.99655\n",
      "Iter 90/100 - Loss: 1.99207\n",
      "Iter 100/100 - Loss: 1.98594\n",
      "Iter 10/100 - Loss: 2.17350\n",
      "Iter 20/100 - Loss: 2.05519\n",
      "Iter 30/100 - Loss: 2.03711\n",
      "Iter 40/100 - Loss: 2.02334\n",
      "Iter 50/100 - Loss: 2.01578\n",
      "Iter 60/100 - Loss: 2.00870\n",
      "Iter 70/100 - Loss: 2.00225\n",
      "Iter 80/100 - Loss: 2.00431\n",
      "Iter 90/100 - Loss: 2.00051\n",
      "Iter 100/100 - Loss: 1.99991\n",
      "Iter 10/100 - Loss: 2.16045\n",
      "Iter 20/100 - Loss: 2.04129\n",
      "Iter 30/100 - Loss: 2.02358\n",
      "Iter 40/100 - Loss: 2.01142\n",
      "Iter 50/100 - Loss: 2.00180\n",
      "Iter 60/100 - Loss: 1.99338\n",
      "Iter 70/100 - Loss: 1.98957\n",
      "Iter 80/100 - Loss: 1.98296\n",
      "Iter 90/100 - Loss: 1.98456\n",
      "Iter 100/100 - Loss: 1.98093\n",
      "Iter 10/100 - Loss: 2.18152\n",
      "Iter 20/100 - Loss: 2.05498\n",
      "Iter 30/100 - Loss: 2.03784\n",
      "Iter 40/100 - Loss: 2.02311\n",
      "Iter 50/100 - Loss: 2.01443\n",
      "Iter 60/100 - Loss: 2.00888\n",
      "Iter 70/100 - Loss: 2.00351\n",
      "Iter 80/100 - Loss: 1.99877\n",
      "Iter 90/100 - Loss: 2.00249\n",
      "Iter 100/100 - Loss: 1.99838\n",
      "Iter 10/100 - Loss: 2.16981\n",
      "Iter 20/100 - Loss: 2.04813\n",
      "Iter 30/100 - Loss: 2.02749\n",
      "Iter 40/100 - Loss: 2.01501\n",
      "Iter 50/100 - Loss: 2.00428\n",
      "Iter 60/100 - Loss: 1.99925\n",
      "Iter 70/100 - Loss: 1.99426\n",
      "Iter 80/100 - Loss: 1.98799\n",
      "Iter 90/100 - Loss: 1.99016\n",
      "Iter 100/100 - Loss: 1.98659\n",
      "Iter 10/100 - Loss: 2.16652\n",
      "Iter 20/100 - Loss: 2.04689\n",
      "Iter 30/100 - Loss: 2.03241\n",
      "Iter 40/100 - Loss: 2.01642\n",
      "Iter 50/100 - Loss: 2.00766\n",
      "Iter 60/100 - Loss: 2.00121\n",
      "Iter 70/100 - Loss: 1.99969\n",
      "Iter 80/100 - Loss: 1.99411\n",
      "Iter 90/100 - Loss: 1.99555\n",
      "Iter 100/100 - Loss: 1.98858\n",
      "Iter 10/100 - Loss: 2.17730\n",
      "Iter 20/100 - Loss: 2.05584\n",
      "Iter 30/100 - Loss: 2.03548\n",
      "Iter 40/100 - Loss: 2.02421\n",
      "Iter 50/100 - Loss: 2.01404\n",
      "Iter 60/100 - Loss: 2.01222\n",
      "Iter 70/100 - Loss: 2.00672\n",
      "Iter 80/100 - Loss: 2.00282\n",
      "Iter 90/100 - Loss: 1.99811\n",
      "Iter 100/100 - Loss: 2.00110\n",
      "Iter 10/100 - Loss: 2.16350\n",
      "Iter 20/100 - Loss: 2.04455\n",
      "Iter 30/100 - Loss: 2.02731\n",
      "Iter 40/100 - Loss: 2.00835\n",
      "Iter 50/100 - Loss: 2.00188\n",
      "Iter 60/100 - Loss: 1.99240\n",
      "Iter 70/100 - Loss: 1.99008\n",
      "Iter 80/100 - Loss: 1.98871\n",
      "Iter 90/100 - Loss: 1.98542\n",
      "Iter 100/100 - Loss: 1.97979\n",
      "Iter 10/100 - Loss: 2.18829\n",
      "Iter 20/100 - Loss: 2.06111\n",
      "Iter 30/100 - Loss: 2.04233\n",
      "Iter 40/100 - Loss: 2.02549\n",
      "Iter 50/100 - Loss: 2.01701\n",
      "Iter 60/100 - Loss: 2.01148\n",
      "Iter 70/100 - Loss: 2.00705\n",
      "Iter 80/100 - Loss: 2.00484\n",
      "Iter 90/100 - Loss: 2.00164\n",
      "Iter 100/100 - Loss: 2.00200\n",
      "Iter 10/100 - Loss: 2.16653\n",
      "Iter 20/100 - Loss: 2.04915\n",
      "Iter 30/100 - Loss: 2.02845\n",
      "Iter 40/100 - Loss: 2.01840\n",
      "Iter 50/100 - Loss: 2.00620\n",
      "Iter 60/100 - Loss: 2.00083\n",
      "Iter 70/100 - Loss: 1.99987\n",
      "Iter 80/100 - Loss: 1.99194\n",
      "Iter 90/100 - Loss: 1.99132\n",
      "Iter 100/100 - Loss: 1.99026\n",
      "Iter 10/100 - Loss: 2.16152\n",
      "Iter 20/100 - Loss: 2.04715\n",
      "Iter 30/100 - Loss: 2.02986\n",
      "Iter 40/100 - Loss: 2.01853\n",
      "Iter 50/100 - Loss: 2.00998\n",
      "Iter 60/100 - Loss: 2.00227\n",
      "Iter 70/100 - Loss: 1.99940\n",
      "Iter 80/100 - Loss: 1.99547\n",
      "Iter 90/100 - Loss: 1.99548\n",
      "Iter 100/100 - Loss: 1.99583\n",
      "Iter 10/100 - Loss: 2.17245\n",
      "Iter 20/100 - Loss: 2.05288\n",
      "Iter 30/100 - Loss: 2.03673\n",
      "Iter 40/100 - Loss: 2.02410\n",
      "Iter 50/100 - Loss: 2.01412\n",
      "Iter 60/100 - Loss: 2.01309\n",
      "Iter 70/100 - Loss: 2.00826\n",
      "Iter 80/100 - Loss: 2.00926\n",
      "Iter 90/100 - Loss: 2.00500\n",
      "Iter 100/100 - Loss: 2.00284\n",
      "Iter 10/100 - Loss: 2.16321\n",
      "Iter 20/100 - Loss: 2.04443\n",
      "Iter 30/100 - Loss: 2.02696\n",
      "Iter 40/100 - Loss: 2.01488\n",
      "Iter 50/100 - Loss: 2.00495\n",
      "Iter 60/100 - Loss: 1.99869\n",
      "Iter 70/100 - Loss: 1.99198\n",
      "Iter 80/100 - Loss: 1.99071\n",
      "Iter 90/100 - Loss: 1.98914\n",
      "Iter 100/100 - Loss: 1.98809\n",
      "Iter 10/100 - Loss: 2.18148\n",
      "Iter 20/100 - Loss: 2.05888\n",
      "Iter 30/100 - Loss: 2.03863\n",
      "Iter 40/100 - Loss: 2.02703\n",
      "Iter 50/100 - Loss: 2.02068\n",
      "Iter 60/100 - Loss: 2.01422\n",
      "Iter 70/100 - Loss: 2.00783\n",
      "Iter 80/100 - Loss: 2.00727\n",
      "Iter 90/100 - Loss: 2.00420\n",
      "Iter 100/100 - Loss: 2.00560\n",
      "Iter 10/100 - Loss: 2.16459\n",
      "Iter 20/100 - Loss: 2.04357\n",
      "Iter 30/100 - Loss: 2.02419\n",
      "Iter 40/100 - Loss: 2.01290\n",
      "Iter 50/100 - Loss: 2.00443\n",
      "Iter 60/100 - Loss: 1.99847\n",
      "Iter 70/100 - Loss: 1.99191\n",
      "Iter 80/100 - Loss: 1.98923\n",
      "Iter 90/100 - Loss: 1.99157\n",
      "Iter 100/100 - Loss: 1.98655\n",
      "Iter 10/100 - Loss: 2.15923\n",
      "Iter 20/100 - Loss: 2.04129\n",
      "Iter 30/100 - Loss: 2.02601\n",
      "Iter 40/100 - Loss: 2.00990\n",
      "Iter 50/100 - Loss: 2.00407\n",
      "Iter 60/100 - Loss: 1.99594\n",
      "Iter 70/100 - Loss: 1.99326\n",
      "Iter 80/100 - Loss: 1.99049\n",
      "Iter 90/100 - Loss: 1.98962\n",
      "Iter 100/100 - Loss: 1.98782\n",
      "Iter 10/100 - Loss: 2.16949\n",
      "Iter 20/100 - Loss: 2.05150\n",
      "Iter 30/100 - Loss: 2.03314\n",
      "Iter 40/100 - Loss: 2.02319\n",
      "Iter 50/100 - Loss: 2.01264\n",
      "Iter 60/100 - Loss: 2.00764\n",
      "Iter 70/100 - Loss: 2.00229\n",
      "Iter 80/100 - Loss: 2.00276\n",
      "Iter 90/100 - Loss: 1.99960\n",
      "Iter 100/100 - Loss: 2.00004\n",
      "Iter 10/100 - Loss: 2.16197\n",
      "Iter 20/100 - Loss: 2.04168\n",
      "Iter 30/100 - Loss: 2.02414\n",
      "Iter 40/100 - Loss: 2.00983\n",
      "Iter 50/100 - Loss: 2.00078\n",
      "Iter 60/100 - Loss: 1.99247\n",
      "Iter 70/100 - Loss: 1.98957\n",
      "Iter 80/100 - Loss: 1.98649\n",
      "Iter 90/100 - Loss: 1.98346\n",
      "Iter 100/100 - Loss: 1.98180\n",
      "Iter 10/100 - Loss: 2.18061\n",
      "Iter 20/100 - Loss: 2.05742\n",
      "Iter 30/100 - Loss: 2.03696\n",
      "Iter 40/100 - Loss: 2.02232\n",
      "Iter 50/100 - Loss: 2.01559\n",
      "Iter 60/100 - Loss: 2.00782\n",
      "Iter 70/100 - Loss: 2.00907\n",
      "Iter 80/100 - Loss: 2.00086\n",
      "Iter 90/100 - Loss: 2.00061\n",
      "Iter 100/100 - Loss: 2.00128\n",
      "Iter 10/100 - Loss: 2.16878\n",
      "Iter 20/100 - Loss: 2.04850\n",
      "Iter 30/100 - Loss: 2.02869\n",
      "Iter 40/100 - Loss: 2.01515\n",
      "Iter 50/100 - Loss: 2.00728\n",
      "Iter 60/100 - Loss: 2.00001\n",
      "Iter 70/100 - Loss: 1.99589\n",
      "Iter 80/100 - Loss: 1.99302\n",
      "Iter 90/100 - Loss: 1.99201\n",
      "Iter 100/100 - Loss: 1.98813\n",
      "Iter 10/100 - Loss: 2.16222\n",
      "Iter 20/100 - Loss: 2.04757\n",
      "Iter 30/100 - Loss: 2.02937\n",
      "Iter 40/100 - Loss: 2.01788\n",
      "Iter 50/100 - Loss: 2.00697\n",
      "Iter 60/100 - Loss: 2.00248\n",
      "Iter 70/100 - Loss: 1.99812\n",
      "Iter 80/100 - Loss: 1.99464\n",
      "Iter 90/100 - Loss: 1.99643\n",
      "Iter 100/100 - Loss: 1.99488\n",
      "Iter 10/100 - Loss: 2.17486\n",
      "Iter 20/100 - Loss: 2.05617\n",
      "Iter 30/100 - Loss: 2.03742\n",
      "Iter 40/100 - Loss: 2.02564\n",
      "Iter 50/100 - Loss: 2.01753\n",
      "Iter 60/100 - Loss: 2.00870\n",
      "Iter 70/100 - Loss: 2.00900\n",
      "Iter 80/100 - Loss: 2.00457\n",
      "Iter 90/100 - Loss: 2.00059\n",
      "Iter 100/100 - Loss: 2.00055\n",
      "Iter 10/100 - Loss: 2.16207\n",
      "Iter 20/100 - Loss: 2.04485\n",
      "Iter 30/100 - Loss: 2.02796\n",
      "Iter 40/100 - Loss: 2.01321\n",
      "Iter 50/100 - Loss: 2.00251\n",
      "Iter 60/100 - Loss: 1.99738\n",
      "Iter 70/100 - Loss: 1.99199\n",
      "Iter 80/100 - Loss: 1.98783\n",
      "Iter 90/100 - Loss: 1.98500\n",
      "Iter 100/100 - Loss: 1.98513\n",
      "Iter 10/100 - Loss: 2.18803\n",
      "Iter 20/100 - Loss: 2.06148\n",
      "Iter 30/100 - Loss: 2.04181\n",
      "Iter 40/100 - Loss: 2.02754\n",
      "Iter 50/100 - Loss: 2.02071\n",
      "Iter 60/100 - Loss: 2.01707\n",
      "Iter 70/100 - Loss: 2.01307\n",
      "Iter 80/100 - Loss: 2.00831\n",
      "Iter 90/100 - Loss: 2.00591\n",
      "Iter 100/100 - Loss: 2.00300\n",
      "Iter 10/100 - Loss: 2.16771\n",
      "Iter 20/100 - Loss: 2.04559\n",
      "Iter 30/100 - Loss: 2.02778\n",
      "Iter 40/100 - Loss: 2.01542\n",
      "Iter 50/100 - Loss: 2.00490\n",
      "Iter 60/100 - Loss: 2.00033\n",
      "Iter 70/100 - Loss: 1.99374\n",
      "Iter 80/100 - Loss: 1.99157\n",
      "Iter 90/100 - Loss: 1.98880\n",
      "Iter 100/100 - Loss: 1.98663\n",
      "Iter 10/100 - Loss: 2.16190\n",
      "Iter 20/100 - Loss: 2.04399\n",
      "Iter 30/100 - Loss: 2.02868\n",
      "Iter 40/100 - Loss: 2.01566\n",
      "Iter 50/100 - Loss: 2.00679\n",
      "Iter 60/100 - Loss: 2.00156\n",
      "Iter 70/100 - Loss: 1.99720\n",
      "Iter 80/100 - Loss: 1.99601\n",
      "Iter 90/100 - Loss: 1.99180\n",
      "Iter 100/100 - Loss: 1.98997\n",
      "Iter 10/100 - Loss: 2.17470\n",
      "Iter 20/100 - Loss: 2.05134\n",
      "Iter 30/100 - Loss: 2.03509\n",
      "Iter 40/100 - Loss: 2.02313\n",
      "Iter 50/100 - Loss: 2.01325\n",
      "Iter 60/100 - Loss: 2.00808\n",
      "Iter 70/100 - Loss: 2.00418\n",
      "Iter 80/100 - Loss: 2.00228\n",
      "Iter 90/100 - Loss: 1.99872\n",
      "Iter 100/100 - Loss: 1.99819\n",
      "Iter 10/100 - Loss: 2.16143\n",
      "Iter 20/100 - Loss: 2.04181\n",
      "Iter 30/100 - Loss: 2.02489\n",
      "Iter 40/100 - Loss: 2.00801\n",
      "Iter 50/100 - Loss: 2.00156\n",
      "Iter 60/100 - Loss: 1.99103\n",
      "Iter 70/100 - Loss: 1.98909\n",
      "Iter 80/100 - Loss: 1.98246\n",
      "Iter 90/100 - Loss: 1.98445\n",
      "Iter 100/100 - Loss: 1.98183\n",
      "Iter 10/100 - Loss: 2.19061\n",
      "Iter 20/100 - Loss: 2.06011\n",
      "Iter 30/100 - Loss: 2.03924\n",
      "Iter 40/100 - Loss: 2.02312\n",
      "Iter 50/100 - Loss: 2.01413\n",
      "Iter 60/100 - Loss: 2.00946\n",
      "Iter 70/100 - Loss: 2.00808\n",
      "Iter 80/100 - Loss: 2.00283\n",
      "Iter 90/100 - Loss: 2.00014\n",
      "Iter 100/100 - Loss: 1.99661\n",
      "Iter 10/100 - Loss: 2.16923\n",
      "Iter 20/100 - Loss: 2.04762\n",
      "Iter 30/100 - Loss: 2.02679\n",
      "Iter 40/100 - Loss: 2.01687\n",
      "Iter 50/100 - Loss: 2.00484\n",
      "Iter 60/100 - Loss: 1.99755\n",
      "Iter 70/100 - Loss: 1.99389\n",
      "Iter 80/100 - Loss: 1.99597\n",
      "Iter 90/100 - Loss: 1.99275\n",
      "Iter 100/100 - Loss: 1.98849\n",
      "Iter 10/100 - Loss: 2.16533\n",
      "Iter 20/100 - Loss: 2.04741\n",
      "Iter 30/100 - Loss: 2.03140\n",
      "Iter 40/100 - Loss: 2.01741\n",
      "Iter 50/100 - Loss: 2.00853\n",
      "Iter 60/100 - Loss: 1.99999\n",
      "Iter 70/100 - Loss: 1.99799\n",
      "Iter 80/100 - Loss: 1.99321\n",
      "Iter 90/100 - Loss: 1.99243\n",
      "Iter 100/100 - Loss: 1.98899\n",
      "Iter 10/100 - Loss: 2.17922\n",
      "Iter 20/100 - Loss: 2.05520\n",
      "Iter 30/100 - Loss: 2.03658\n",
      "Iter 40/100 - Loss: 2.02254\n",
      "Iter 50/100 - Loss: 2.01819\n",
      "Iter 60/100 - Loss: 2.00964\n",
      "Iter 70/100 - Loss: 2.00542\n",
      "Iter 80/100 - Loss: 2.00342\n",
      "Iter 90/100 - Loss: 2.00261\n",
      "Iter 100/100 - Loss: 2.00060\n",
      "Iter 10/100 - Loss: 2.16255\n",
      "Iter 20/100 - Loss: 2.04453\n",
      "Iter 30/100 - Loss: 2.02813\n",
      "Iter 40/100 - Loss: 2.01291\n",
      "Iter 50/100 - Loss: 2.00241\n",
      "Iter 60/100 - Loss: 1.99525\n",
      "Iter 70/100 - Loss: 1.99221\n",
      "Iter 80/100 - Loss: 1.98680\n",
      "Iter 90/100 - Loss: 1.98380\n",
      "Iter 100/100 - Loss: 1.98731\n",
      "Iter 10/100 - Loss: 2.18856\n",
      "Iter 20/100 - Loss: 2.06003\n",
      "Iter 30/100 - Loss: 2.04137\n",
      "Iter 40/100 - Loss: 2.02492\n",
      "Iter 50/100 - Loss: 2.01792\n",
      "Iter 60/100 - Loss: 2.01199\n",
      "Iter 70/100 - Loss: 2.00850\n",
      "Iter 80/100 - Loss: 2.00520\n",
      "Iter 90/100 - Loss: 2.00397\n",
      "Iter 100/100 - Loss: 2.00411\n",
      "Iter 10/100 - Loss: 2.16363\n",
      "Iter 20/100 - Loss: 2.04463\n",
      "Iter 30/100 - Loss: 2.02626\n",
      "Iter 40/100 - Loss: 2.01281\n",
      "Iter 50/100 - Loss: 2.00342\n",
      "Iter 60/100 - Loss: 1.99967\n",
      "Iter 70/100 - Loss: 1.99549\n",
      "Iter 80/100 - Loss: 1.98558\n",
      "Iter 90/100 - Loss: 1.98790\n",
      "Iter 100/100 - Loss: 1.98786\n",
      "Iter 10/100 - Loss: 2.15911\n",
      "Iter 20/100 - Loss: 2.04548\n",
      "Iter 30/100 - Loss: 2.02487\n",
      "Iter 40/100 - Loss: 2.01184\n",
      "Iter 50/100 - Loss: 2.00397\n",
      "Iter 60/100 - Loss: 2.00009\n",
      "Iter 70/100 - Loss: 1.99560\n",
      "Iter 80/100 - Loss: 1.99240\n",
      "Iter 90/100 - Loss: 1.99176\n",
      "Iter 100/100 - Loss: 1.98561\n",
      "Iter 10/100 - Loss: 2.17355\n",
      "Iter 20/100 - Loss: 2.05112\n",
      "Iter 30/100 - Loss: 2.03610\n",
      "Iter 40/100 - Loss: 2.02311\n",
      "Iter 50/100 - Loss: 2.01658\n",
      "Iter 60/100 - Loss: 2.00805\n",
      "Iter 70/100 - Loss: 2.00637\n",
      "Iter 80/100 - Loss: 2.00265\n",
      "Iter 90/100 - Loss: 2.00135\n",
      "Iter 100/100 - Loss: 1.99969\n",
      "Iter 10/100 - Loss: 2.15905\n",
      "Iter 20/100 - Loss: 2.04104\n",
      "Iter 30/100 - Loss: 2.02381\n",
      "Iter 40/100 - Loss: 2.01083\n",
      "Iter 50/100 - Loss: 2.00127\n",
      "Iter 60/100 - Loss: 1.99332\n",
      "Iter 70/100 - Loss: 1.98793\n",
      "Iter 80/100 - Loss: 1.98881\n",
      "Iter 90/100 - Loss: 1.98548\n",
      "Iter 100/100 - Loss: 1.98437\n",
      "Iter 10/100 - Loss: 2.18236\n",
      "Iter 20/100 - Loss: 2.05895\n",
      "Iter 30/100 - Loss: 2.03815\n",
      "Iter 40/100 - Loss: 2.02602\n",
      "Iter 50/100 - Loss: 2.01566\n",
      "Iter 60/100 - Loss: 2.01211\n",
      "Iter 70/100 - Loss: 2.00703\n",
      "Iter 80/100 - Loss: 2.00537\n",
      "Iter 90/100 - Loss: 2.00174\n",
      "Iter 100/100 - Loss: 2.00137\n",
      "Iter 10/100 - Loss: 2.16876\n",
      "Iter 20/100 - Loss: 2.04574\n",
      "Iter 30/100 - Loss: 2.02634\n",
      "Iter 40/100 - Loss: 2.01268\n",
      "Iter 50/100 - Loss: 2.00190\n",
      "Iter 60/100 - Loss: 1.99871\n",
      "Iter 70/100 - Loss: 1.99047\n",
      "Iter 80/100 - Loss: 1.99058\n",
      "Iter 90/100 - Loss: 1.98121\n",
      "Iter 100/100 - Loss: 1.98488\n",
      "Iter 10/100 - Loss: 2.16489\n",
      "Iter 20/100 - Loss: 2.04485\n",
      "Iter 30/100 - Loss: 2.02452\n",
      "Iter 40/100 - Loss: 2.01414\n",
      "Iter 50/100 - Loss: 2.00496\n",
      "Iter 60/100 - Loss: 1.99827\n",
      "Iter 70/100 - Loss: 1.99509\n",
      "Iter 80/100 - Loss: 1.99356\n",
      "Iter 90/100 - Loss: 1.99235\n",
      "Iter 100/100 - Loss: 1.98695\n",
      "Iter 10/100 - Loss: 2.17525\n",
      "Iter 20/100 - Loss: 2.05364\n",
      "Iter 30/100 - Loss: 2.03663\n",
      "Iter 40/100 - Loss: 2.02362\n",
      "Iter 50/100 - Loss: 2.01506\n",
      "Iter 60/100 - Loss: 2.00902\n",
      "Iter 70/100 - Loss: 2.00643\n",
      "Iter 80/100 - Loss: 2.00367\n",
      "Iter 90/100 - Loss: 1.99754\n",
      "Iter 100/100 - Loss: 2.00135\n",
      "Iter 10/100 - Loss: 2.16409\n",
      "Iter 20/100 - Loss: 2.04335\n",
      "Iter 30/100 - Loss: 2.02356\n",
      "Iter 40/100 - Loss: 2.01032\n",
      "Iter 50/100 - Loss: 2.00079\n",
      "Iter 60/100 - Loss: 1.99130\n",
      "Iter 70/100 - Loss: 1.98608\n",
      "Iter 80/100 - Loss: 1.98352\n",
      "Iter 90/100 - Loss: 1.98475\n",
      "Iter 100/100 - Loss: 1.97866\n",
      "Iter 10/100 - Loss: 2.18743\n",
      "Iter 20/100 - Loss: 2.06053\n",
      "Iter 30/100 - Loss: 2.03904\n",
      "Iter 40/100 - Loss: 2.02610\n",
      "Iter 50/100 - Loss: 2.01592\n",
      "Iter 60/100 - Loss: 2.01218\n",
      "Iter 70/100 - Loss: 2.00469\n",
      "Iter 80/100 - Loss: 2.00526\n",
      "Iter 90/100 - Loss: 2.00031\n",
      "Iter 100/100 - Loss: 2.00223\n",
      "Iter 10/100 - Loss: 2.17041\n",
      "Iter 20/100 - Loss: 2.04521\n",
      "Iter 30/100 - Loss: 2.02609\n",
      "Iter 40/100 - Loss: 2.01237\n",
      "Iter 50/100 - Loss: 2.00419\n",
      "Iter 60/100 - Loss: 1.99617\n",
      "Iter 70/100 - Loss: 1.99615\n",
      "Iter 80/100 - Loss: 1.99299\n",
      "Iter 90/100 - Loss: 1.98749\n",
      "Iter 100/100 - Loss: 1.98310\n",
      "Iter 10/100 - Loss: 2.16391\n",
      "Iter 20/100 - Loss: 2.04555\n",
      "Iter 30/100 - Loss: 2.02548\n",
      "Iter 40/100 - Loss: 2.01222\n",
      "Iter 50/100 - Loss: 2.00281\n",
      "Iter 60/100 - Loss: 1.99793\n",
      "Iter 70/100 - Loss: 1.99499\n",
      "Iter 80/100 - Loss: 1.99059\n",
      "Iter 90/100 - Loss: 1.98996\n",
      "Iter 100/100 - Loss: 1.98715\n",
      "Iter 10/100 - Loss: 2.17489\n",
      "Iter 20/100 - Loss: 2.05225\n",
      "Iter 30/100 - Loss: 2.03433\n",
      "Iter 40/100 - Loss: 2.02428\n",
      "Iter 50/100 - Loss: 2.01301\n",
      "Iter 60/100 - Loss: 2.00598\n",
      "Iter 70/100 - Loss: 2.00489\n",
      "Iter 80/100 - Loss: 2.00084\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "os.system('mkdir -p data/boruta')\n",
    "for r in range(data_x.shape[1]):\n",
    "    tmp_data_x = np.copy(data_x)\n",
    "    random_list = random.choices(sorted(np.unique(tmp_data_x[:, r])), k=ndata)\n",
    "    tmp_data_x[:, r] = random_list\n",
    "    # pred_y, lower_y, upper_y = simple_gp.solve(tmp_data_x, data_y, 5, 5)\n",
    "    pred_y, lower_y, upper_y = simple_gp.solve(tmp_data_x, data_y, 5, 100)\n",
    "    tmp_df = df.copy()\n",
    "    tmp_df['rand_items'] = random_list\n",
    "    tmp_df['pred_ΔΔG'] = pred_y\n",
    "    tmp_df['lower_ΔΔG'] = lower_y\n",
    "    tmp_df['upper_ΔΔG'] = upper_y\n",
    "    tmp_df.to_csv('data/boruta/pred_DDG_Dataset_gpytorch_Kidera_{}.csv'.format(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
